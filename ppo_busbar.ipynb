{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "671474bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc0dcc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BusbarIESEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    一个简化的母线制电热综合能源系统环境 (IES)\n",
    "    - State: [时刻(0-23), 电价, 电负荷, 热负荷]\n",
    "    - Action: [CHP出力(0-1), EB出力(0-1)] (连续动作)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BusbarIESEnv, self).__init__()\n",
    "\n",
    "        # --- 定义动作空间和状态空间 ---\n",
    "        # 动作空间: 2个连续动作，均归一化到[-1, 1]\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n",
    "\n",
    "        # 状态空间: 4个连续值\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0, 0, 0, 0]),\n",
    "            high=np.array([23, 1.5, 500, 500]),  # 假设电价、负荷上限\n",
    "            shape=(4,),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        # --- 系统参数 (简化) ---\n",
    "        self.gas_price = 0.35  # 元/kWh\n",
    "        self.chp_max_power = 300  # kW\n",
    "        self.chp_elec_efficiency = 0.4  # 发电效率\n",
    "        self.chp_heat_efficiency = 0.5  # 余热回收效率\n",
    "        self.eb_max_power = 200  # kW\n",
    "        self.eb_efficiency = 0.98\n",
    "\n",
    "        # --- 模拟数据 (简化) ---\n",
    "        # 预设一天24小时的负荷和电价曲线\n",
    "        self.elec_prices = np.array(\n",
    "            [\n",
    "                0.4,\n",
    "                0.4,\n",
    "                0.4,\n",
    "                0.4,\n",
    "                0.4,\n",
    "                0.7,\n",
    "                0.7,\n",
    "                1.2,\n",
    "                1.2,\n",
    "                1.2,\n",
    "                0.7,\n",
    "                0.7,\n",
    "                1.2,\n",
    "                1.2,\n",
    "                0.7,\n",
    "                0.7,\n",
    "                0.7,\n",
    "                1.2,\n",
    "                1.2,\n",
    "                1.2,\n",
    "                0.7,\n",
    "                0.7,\n",
    "                0.4,\n",
    "                0.4,\n",
    "            ]\n",
    "        )\n",
    "        self.elec_loads = np.array(\n",
    "            [\n",
    "                100,\n",
    "                100,\n",
    "                100,\n",
    "                110,\n",
    "                120,\n",
    "                150,\n",
    "                200,\n",
    "                300,\n",
    "                320,\n",
    "                350,\n",
    "                300,\n",
    "                250,\n",
    "                220,\n",
    "                220,\n",
    "                250,\n",
    "                280,\n",
    "                330,\n",
    "                400,\n",
    "                420,\n",
    "                380,\n",
    "                300,\n",
    "                250,\n",
    "                180,\n",
    "                120,\n",
    "            ]\n",
    "        )\n",
    "        self.heat_loads = np.array(\n",
    "            [\n",
    "                150,\n",
    "                150,\n",
    "                140,\n",
    "                140,\n",
    "                160,\n",
    "                180,\n",
    "                220,\n",
    "                280,\n",
    "                300,\n",
    "                300,\n",
    "                280,\n",
    "                250,\n",
    "                230,\n",
    "                220,\n",
    "                230,\n",
    "                260,\n",
    "                300,\n",
    "                350,\n",
    "                360,\n",
    "                320,\n",
    "                280,\n",
    "                240,\n",
    "                200,\n",
    "                160,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = 0\n",
    "        obs = self._get_obs()\n",
    "        info = {}\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        # --- 1. 将归一化的动作转换为实际物理值 ---\n",
    "        # tanh激活函数输出在[-1, 1], 我们把它映射到[0, 1]\n",
    "        chp_output_ratio = (action[0] + 1) / 2\n",
    "        eb_output_ratio = (action[1] + 1) / 2\n",
    "\n",
    "        # --- 2. 计算能源产出 ---\n",
    "        chp_gas_consumption = (\n",
    "            self.chp_max_power * chp_output_ratio\n",
    "        ) / self.chp_elec_efficiency\n",
    "        chp_elec_production = self.chp_max_power * chp_output_ratio\n",
    "        chp_heat_production = chp_gas_consumption * self.chp_heat_efficiency\n",
    "\n",
    "        eb_elec_consumption = self.eb_max_power * eb_output_ratio\n",
    "        eb_heat_production = eb_elec_consumption * self.eb_efficiency\n",
    "\n",
    "        # --- 3. 进行能量平衡计算 ---\n",
    "        # 热平衡 (热负荷必须满足，不足则惩罚)\n",
    "        total_heat_production = chp_heat_production + eb_heat_production\n",
    "        heat_load = self.heat_loads[self.current_step]\n",
    "        heat_mismatch = heat_load - total_heat_production\n",
    "\n",
    "        # 电平衡\n",
    "        elec_load = self.elec_loads[self.current_step]\n",
    "        total_elec_consumption = elec_load + eb_elec_consumption\n",
    "        elec_from_grid = total_elec_consumption - chp_elec_production\n",
    "\n",
    "        # --- 4. 计算成本和奖励 ---\n",
    "        grid_price = self.elec_prices[self.current_step]\n",
    "\n",
    "        # a. 能源成本\n",
    "        gas_cost = chp_gas_consumption * self.gas_price\n",
    "        elec_cost = max(0, elec_from_grid) * grid_price  # 只有买电才花钱\n",
    "\n",
    "        # b. 售电收入\n",
    "        elec_revenue = -min(0, elec_from_grid) * grid_price * 0.8  # 卖电收入打8折\n",
    "\n",
    "        # c. 热负荷惩罚 (关键!)\n",
    "        heat_penalty = max(0, heat_mismatch) * 999  # 严重惩罚供热不足\n",
    "\n",
    "        # d. 总奖励 (目标是最大化奖励，所以成本和惩罚是负的)\n",
    "        reward = elec_revenue - gas_cost - elec_cost - heat_penalty\n",
    "\n",
    "        # --- 5. 更新状态 ---\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= 23  # 一天24小时结束\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        info = {}  # 可以用来返回调试信息\n",
    "\n",
    "        # Gymnasium API返回5个值\n",
    "        return obs, reward, done, False, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # 获取当前状态\n",
    "        hour = self.current_step\n",
    "        price = self.elec_prices[hour]\n",
    "        elec_load = self.elec_loads[hour]\n",
    "        heat_load = self.heat_loads[hour]\n",
    "        return np.array([hour, price, elec_load, heat_load], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3ef1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\App\\anaconda\\Lib\\site-packages\\gymnasium\\spaces\\box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "--- 开始训练PPO模型 ---\n",
      "Logging to ./ppo_ies_tensorboard/PPO_1\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 23        |\n",
      "|    ep_rew_mean     | -1.14e+06 |\n",
      "| time/              |           |\n",
      "|    fps             | 2275      |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 3         |\n",
      "|    total_timesteps | 8192      |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 23          |\n",
      "|    ep_rew_mean          | -1.08e+06   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 796         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000457996 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | -3.58e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.66e+11    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00323    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 2.77e+11    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 23           |\n",
      "|    ep_rew_mean          | -1.04e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 644          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 38           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005112997 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.07e+11     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00343     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.39e+11     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 23           |\n",
      "|    ep_rew_mean          | -1.07e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 678          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 48           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005680019 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.14e+11     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00368     |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 2.45e+11     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 23            |\n",
      "|    ep_rew_mean          | -9.52e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 739           |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 55            |\n",
      "|    total_timesteps      | 40960         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00073739677 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.12e+11      |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.00449      |\n",
      "|    std                  | 0.999         |\n",
      "|    value_loss           | 2.19e+11      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 23           |\n",
      "|    ep_rew_mean          | -8.92e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 761          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 64           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008232575 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.02e+11     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00475     |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 1.85e+11     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 23           |\n",
      "|    ep_rew_mean          | -8.36e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 811          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 70           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008604446 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.49e+10     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00479     |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 1.8e+11      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 23           |\n",
      "|    ep_rew_mean          | -7.57e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 75           |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009855509 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.28e+10     |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00507     |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 1.6e+11      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 23           |\n",
      "|    ep_rew_mean          | -7.6e+05     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 927          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 79           |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009870521 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.3e+10      |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00494     |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 1.51e+11     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 23          |\n",
      "|    ep_rew_mean          | -6.5e+05    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 974         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 84          |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001130543 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.8e+10     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00543    |\n",
      "|    std                  | 0.997       |\n",
      "|    value_loss           | 1.37e+11    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 23          |\n",
      "|    ep_rew_mean          | -6.68e+05   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1015        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 88          |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001511639 |\n",
      "|    clip_fraction        | 2.44e-05    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.91e+10    |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00628    |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 1.11e+11    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 23           |\n",
      "|    ep_rew_mean          | -5.74e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1054         |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 93           |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014544367 |\n",
      "|    clip_fraction        | 1.22e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.23e+10     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00595     |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 1.02e+11     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 23           |\n",
      "|    ep_rew_mean          | -5.16e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1089         |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 97           |\n",
      "|    total_timesteps      | 106496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019685319 |\n",
      "|    clip_fraction        | 0.000122     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.55e+10     |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00711     |\n",
      "|    std                  | 0.994        |\n",
      "|    value_loss           | 8.74e+10     |\n",
      "------------------------------------------\n",
      "--- 训练完成 ---\n",
      "\n",
      "--- 评估训练好的策略 ---\n",
      "3天模拟的总奖励(越高越好，代表成本越低): -23367.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\App\\anaconda\\Lib\\site-packages\\gymnasium\\spaces\\box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# 1. 创建并向量化环境\n",
    "# 向量化是PPO等算法高效采样的关键\n",
    "vec_env = make_vec_env(BusbarIESEnv, n_envs=4)  # 使用4个并行环境来加速采样\n",
    "\n",
    "# 2. 创建PPO模型\n",
    "# \"MlpPolicy\" 表示使用多层感知机作为Actor和Critic的网络结构\n",
    "model = PPO(\"MlpPolicy\", vec_env, verbose=1, tensorboard_log=\"./ppo_ies_tensorboard/\")\n",
    "\n",
    "# 3. 开始训练！\n",
    "# SB3会自动处理PPO的“收集数据->更新->丢弃数据”的循环\n",
    "print(\"--- 开始训练PPO模型 ---\")\n",
    "model.learn(total_timesteps=500_000)\n",
    "print(\"--- 训练完成 ---\")\n",
    "\n",
    "# 4. 评估训练好的模型\n",
    "print(\"\\n--- 评估训练好的策略 ---\")\n",
    "eval_env = BusbarIESEnv()\n",
    "obs, _ = eval_env.reset()\n",
    "total_reward = 0\n",
    "for day in range(3):  # 模拟评估3天\n",
    "    obs, _ = eval_env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        # deterministic=True表示在评估时不进行随机探索\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, _, info = eval_env.step(action)\n",
    "        total_reward += reward\n",
    "print(f\"3天模拟的总奖励(越高越好，代表成本越低): {total_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a117823b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
